{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A facial recognition system is a technology capable of identifying or verifying a person from a digital image or a video frame from a video source\n",
    "For this section, we will be using facial recognition to **identify perpetrators or persons involved in the crime(s)** detected by our smart surveillance system\n",
    " ***There are three easy steps to build a facial recognition system, which are similar to the steps that our brains use for recognizing faces. These steps are:***\n",
    "\n",
    "*   **Data Gathering:** Gather face data (face images in this case) of the persons you want to identify.\n",
    "*   **Train the Recognizer:** Feed that face data and respective names of each face to the recognizer so that it can learn.\n",
    "*  **Recognition:** Feed new faces of that people and see if the face recognizer you just trained recognizes them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries to be imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'face_recognition'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-c6f5f26c8d52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mface_recognition\u001b[0m \u001b[1;31m#To perform the facial recognition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m \u001b[1;31m#for live or fixed facial detection and recognition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m \u001b[1;31m#To navigate through directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m \u001b[1;31m#creating labels for image arrays to train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;31m#Image processing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'face_recognition'"
     ]
    }
   ],
   "source": [
    "import face_recognition #To perform the facial recognition\n",
    "import cv2 #for live or fixed facial detection and recognition\n",
    "import os #To navigate through directory\n",
    "import pickle #creating labels for image arrays to train the model\n",
    "import PIL #Image processing\n",
    "from PIL import Image #For image processing \n",
    "import numpy as np #Creating image arrays as OpenCV face recognizer requires it to perform facial recognition\n",
    "import matplotlib.pyplot as plt #plot rectangles around faces detected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing and Preparing the training data (face images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Facesss = [] #Empty list to append face data to later for model training\n",
    "Face_labels = [] #empty list the face labels (names of persons to be identified) will be appended to\n",
    "currentID = 0 #openCV requires labels to be in integer format. ID 0 is empty\n",
    "label_ids = {}\n",
    "face_cascade = cv2.CascadeClassifier(\"C:/Users/aiden/Desktop/Facial Recognition/data/haarcascade_frontalface_alt2.xml\") #cascade used to detect faces, especially from profile view\n",
    "\n",
    "data = \"C:/Users/aiden/Desktop/Facial Recognition/Train_Datasets\" #Directory you are working in\n",
    "#IMG_DIR = os.path.join(data, \"\") #Where each person's faces data (images) are stored\n",
    "\n",
    "#iterate through directory for training images which needs to be preprocessed before fed into model\n",
    "for root, dirs, files in os.walk(data):\n",
    "    for file in files:\n",
    "        if file.endswith(\"jpg\") or file.endswith(\"png\"):\n",
    "            PATH = os.path.join(root, file)\n",
    "            label = os.path.basename(root) #label = name of folder (Name of person)\n",
    "            #subjects.append(label)\n",
    "            if not label in label_ids:\n",
    "                label_ids[label] = currentID\n",
    "                currentID +=1 #maps names of persons to ID e.g. Aiden = 1, Taryn = 2, etc\n",
    "            id_ = label_ids[label]\n",
    "                \n",
    "            PIL_IMG = Image.open(PATH).convert('L') #GRAYSCALE as OpenCV requires the image to be in grayscale format\n",
    "            size = (600, 600) #resize images to all be the same size\n",
    "            Final_IMG = PIL_IMG.resize(size, Image.ANTIALIAS)\n",
    "            IMG_ARRAY = np.array(Final_IMG, \"uint8\")\n",
    "            \n",
    "            Faces = face_cascade.detectMultiScale(IMG_ARRAY, \n",
    "                                          scaleFactor = 1.3,\n",
    "                                          minNeighbors = 5\n",
    "                                                 )\n",
    "                                          \n",
    "            \n",
    "            for (x,y,w,h) in Faces:\n",
    "                roi = IMG_ARRAY[y:y+h, x:x+w] #region of interest: face. only the area in which a face is detected will be focused on (crop out rest of image not containing a face)\n",
    "                Facesss.append(roi) #append faces to list\n",
    "                Face_labels.append(id_) #append face labels\n",
    "\n",
    "with open(\"labels.pickle\", 'wb')as f:\n",
    "    pickle.dump(label_ids, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Face_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Aiden_Titus': 0,\n",
       " 'Anthea_Michael': 1,\n",
       " 'Ariel_Sharon': 2,\n",
       " 'Arnold_Schwarzenegger': 3,\n",
       " 'Colin_Powell': 4,\n",
       " 'Donald_Rumsfeld': 5,\n",
       " 'George_W_Bush': 6,\n",
       " 'Gerhard_Schroeder': 7,\n",
       " 'Hugo_Chavez': 8,\n",
       " 'Jacques_Chirac': 9,\n",
       " 'Kayla': 10,\n",
       " 'Taryn_Michael': 11,\n",
       " 'Tony_Blair': 12,\n",
       " 'Vladimir_Putin': 13}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Face Recognizer\n",
    "\n",
    "OpenCV has 3 built-in face recognizer functions:\n",
    "\n",
    "\n",
    "*   **EigenFaces**: cv2.face.createEigenFaceRecognizer()\n",
    "\n",
    "*   **FisherFaces**: cv2.face.createFisherFaceRecognizer()\n",
    "\n",
    "*   **Local Binary Patterns Histograms (LBPH)**: cv2.face.createLBPHFaceRecognizer()\n",
    "\n",
    "\n",
    "We will be using the LBPH recognizer as its ability to recognize faces is *not hindered by changes in light conditions*, as is the case with EigenFaces and FisherFaces. This is useful as we cannot always guarentee good lighting conditions due to the placement of our surveilance cameras, weather conditions, different times of the day, camera quality, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2.cv2' has no attribute 'LBPHFaceRecognizer_create'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-7e195131d64a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mRecog\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLBPHFaceRecognizer_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Using LBPH algorithm to perform the facial recognition. use this command to create recognizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mRecog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFacesss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFace_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Train recognizer using face image array and labels array created\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mRecog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"trainDATA.yml\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Save the model in yml format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2.cv2' has no attribute 'LBPHFaceRecognizer_create'"
     ]
    }
   ],
   "source": [
    "Recog = cv2.face.LBPHFaceRecognizer_create() #Using LBPH algorithm to perform the facial recognition. use this command to create recognizer\n",
    "Recog.train(Facesss, np.array(Face_labels)) #Train recognizer using face image array and labels array created\n",
    "Recog.save(\"trainDATA.yml\") #Save the model in yml format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    face_cascade = cv2.CascadeClassifier(\"C:/Users/aiden/Desktop/Facial Recognition/data/haarcascade_frontalface_alt2.xml\")\n",
    "    Faces = face_cascade.detectMultiScale(IMG_ARRAY, \n",
    "                                          scaleFactor = 1.3,\n",
    "                                          minNeighbors = 5\n",
    "                                                 )\n",
    "    if (len(Faces) == 0):\n",
    "        return None, None\n",
    "    \n",
    "    (x, y, w, h) = Faces[0]\n",
    "    \n",
    "    return gray[y:y+w, x:x+h], Faces[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the facial detection and recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 25.379426817925086)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image = Image.open(\"C:/Users/aiden/Desktop/Facial Recognition/Test_Data/0.jpg\").convert(\"L\")\n",
    "Recog.predict(np.array(test_image))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "Recog.getThreshold()\n",
    "Recog.setThreshold(1.7976931348623157e+306)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.image import imread\n",
    "\n",
    " \n",
    "RESULTS = cv2.face.StandardCollector_create(threshold = 2000)\n",
    "test_data = \"C:/Users/aiden/Desktop/Facial Recognition/Test_Data\"\n",
    "for root, dirs, files in os.walk(test_data):\n",
    "  \n",
    "    for file in files:\n",
    "        if file.endswith(\"jpg\") or file.endswith(\"png\"):\n",
    "            PATH_test = os.path.join(root, file)\n",
    "            image  = Image.open(PATH_test).convert(\"L\")#GRAYSCALE as OpenCV requires the image to be in grayscale format\n",
    "            TEST_ARRAY = np.array(Final_test, \"uint8\") \n",
    "            face_test = face_cascade.detectMultiScale(TEST_ARRAY)\n",
    "            for (x,y,w,h) in face_test:\n",
    "                \n",
    "            \n",
    "            \n",
    "                Recog.predict_collect(TEST_ARRAY, RESULTS)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 26.540615417483348),\n",
       " (2, 28.950161658971414),\n",
       " (2, 29.045802082306952),\n",
       " (3, 29.414033536995504),\n",
       " (2, 29.651038293487463),\n",
       " (2, 29.815891008224753),\n",
       " (2, 30.260818658515156),\n",
       " (2, 31.28495345481176),\n",
       " (2, 31.66649442964014),\n",
       " (2, 31.951264977921277),\n",
       " (2, 32.65329041710134),\n",
       " (2, 33.01652932483319),\n",
       " (1, 33.30515208034388),\n",
       " (1, 33.940524034362326),\n",
       " (2, 34.930894516774885),\n",
       " (1, 34.954099043110475),\n",
       " (2, 34.960710988908964),\n",
       " (1, 35.80798346411237),\n",
       " (2, 36.17167592452485),\n",
       " (1, 36.71761206100367),\n",
       " (1, 36.913680271316196),\n",
       " (1, 36.97313995714909),\n",
       " (2, 37.643191577161566),\n",
       " (1, 37.82953048059263),\n",
       " (2, 38.770179247658426),\n",
       " (3, 39.03673466902148),\n",
       " (1, 39.19173794101303),\n",
       " (3, 39.2407472367063),\n",
       " (1, 39.25696917127059),\n",
       " (1, 39.307056195764225),\n",
       " (3, 39.31865213222187),\n",
       " (3, 39.53452401955097),\n",
       " (3, 39.56081340779592),\n",
       " (1, 39.760549247130264),\n",
       " (1, 39.96442010573746),\n",
       " (1, 40.0337043175033),\n",
       " (1, 40.04368574440162),\n",
       " (1, 40.34101299109789),\n",
       " (3, 40.45467619353549),\n",
       " (3, 40.53164693789222),\n",
       " (3, 40.7816415249552),\n",
       " (1, 40.840378641995756),\n",
       " (0, 40.86485467968404),\n",
       " (1, 41.023084104446326),\n",
       " (2, 41.1065939809882),\n",
       " (1, 41.326788866986966),\n",
       " (1, 41.36885493491426),\n",
       " (3, 41.45550070590001),\n",
       " (3, 41.65183048155941),\n",
       " (3, 41.843558082707226),\n",
       " (1, 41.9086951620595),\n",
       " (3, 42.0132973272764),\n",
       " (1, 42.05861912637093),\n",
       " (3, 42.71154414468871),\n",
       " (3, 43.00105863569904),\n",
       " (3, 43.039726635495924),\n",
       " (1, 43.442793839468344),\n",
       " (1, 44.63106722177507),\n",
       " (3, 45.09564562714998),\n",
       " (3, 45.148369920510056),\n",
       " (1, 45.16484671234075),\n",
       " (1, 45.2412677184919),\n",
       " (1, 45.844699984030655),\n",
       " (1, 46.49265162286451),\n",
       " (3, 47.64555547107119),\n",
       " (3, 47.749314194674426),\n",
       " (3, 51.21163268683363),\n",
       " (3, 51.4649563785608),\n",
       " (0, 51.6465870588447),\n",
       " (3, 52.42851590216212),\n",
       " (3, 53.33431540485213),\n",
       " (3, 53.379560925424535),\n",
       " (3, 54.66877928197375),\n",
       " (1, 62.793023973999944),\n",
       " (3, 62.92346315638093),\n",
       " (3, 63.2556771968445),\n",
       " (3, 65.75524800265134),\n",
       " (3, 70.93222153697651),\n",
       " (1, 83.01004580557942)]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULTS.getResults(sorted = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_results = pd.DataFrame(RES)\n",
    "test_results\n",
    "test_results.rename(columns = {'0':'Label', '1':'Confidence'}, inplace = True)\n",
    "test_results.to_csv(\"C:/Users/aiden/Desktop/Facial Recognition/MODEL_RESULTS_309.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>26.540615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>28.950162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>29.045802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>29.414034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>29.651038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>3</td>\n",
       "      <td>62.923463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>3</td>\n",
       "      <td>63.255677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>3</td>\n",
       "      <td>65.755248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>3</td>\n",
       "      <td>70.932222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1</td>\n",
       "      <td>83.010046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0          1\n",
       "0   2  26.540615\n",
       "1   2  28.950162\n",
       "2   2  29.045802\n",
       "3   3  29.414034\n",
       "4   2  29.651038\n",
       ".. ..        ...\n",
       "74  3  62.923463\n",
       "75  3  63.255677\n",
       "76  3  65.755248\n",
       "77  3  70.932222\n",
       "78  1  83.010046\n",
       "\n",
       "[79 rows x 2 columns]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
